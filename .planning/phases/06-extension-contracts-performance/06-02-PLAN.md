---
phase: 06-extension-contracts-performance
plan: 02
type: execute
wave: 2
depends_on:
  - "06-01"
files_modified:
  - projects/cli/cli/config.py
  - projects/cli/cli/main.py
  - projects/cli/README.md
  - projects/cli/settings_mythic.yaml
  - projects/cli/settings_outflank.yaml
  - projects/cli/settings_cobaltstrike.yaml
  - docs/performance.md
  - projects/file_enrichment/tests/benchmarks/README.md
  - projects/file_enrichment/tests/benchmarks/bench_basic_analysis.py
files-modified: [projects/cli/cli/config.py, projects/cli/cli/main.py, projects/cli/README.md, projects/cli/settings_mythic.yaml, projects/cli/settings_outflank.yaml, projects/cli/settings_cobaltstrike.yaml, docs/performance.md, projects/file_enrichment/tests/benchmarks/README.md, projects/file_enrichment/tests/benchmarks/bench_basic_analysis.py]
autonomous: true
objective: "Add connector onboarding preflight validation guidance and throughput baseline/tuning guardrails so extension onboarding failures and performance regressions are caught early."
requirements:
  - EXT-02
user_setup: []
must_haves:
  truths:
    - "Connector onboarding includes explicit schema/config preflight validation steps that detect miswiring before long-running connector sync starts."
    - "Workflow throughput baseline capture is measurable and repeatable with explicit benchmark commands and compare workflow guidance."
    - "Performance tuning guidance links queue-level bottlenecks to concrete, benchmark-backed tuning decisions without claiming unsupported end-to-end gains."
  artifacts:
    - path: "projects/cli/cli/config.py"
      provides: "connector config validation contract and/or explicit preflight validation helpers"
    - path: "projects/cli/README.md"
      provides: "operator-facing connector onboarding preflight runbook"
      min_lines: 200
    - path: "projects/cli/settings_outflank.yaml"
      provides: "schema-key aligned example configuration reducing onboarding miswiring"
    - path: "docs/performance.md"
      provides: "throughput baseline/tuning guardrail guidance with measurable command references"
      min_lines: 140
    - path: "projects/file_enrichment/tests/benchmarks/README.md"
      provides: "benchmark baseline/save/compare runbook aligned to throughput tuning"
  key_links:
    - from: "projects/cli/README.md"
      to: "projects/cli/cli/config.py"
      via: "onboarding guide references concrete validation contract and failure handling"
    - from: "projects/cli/settings_outflank.yaml"
      to: "projects/cli/cli/config.py"
      via: "example keys and comments align to actual model fields"
    - from: "docs/performance.md"
      to: "projects/file_enrichment/tests/benchmarks/README.md"
      via: "performance tuning guidance references repeatable benchmark baseline and compare workflow"
---

<objective>
Add connector onboarding preflight validation guidance and throughput baseline/tuning guardrails so extension onboarding failures and performance regressions are caught early.

Purpose: satisfy EXT-02 while enforcing measurable workflow-throughput baselining and tuning guardrails.
Output: connector validation runbook plus benchmark-backed throughput guidance updates.
</objective>

<execution_context>
@/Users/maleick/.codex/get-shit-done/workflows/execute-plan.md
@/Users/maleick/.codex/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/STATE.md
@.planning/phases/06-extension-contracts-performance/06-RESEARCH.md
@.planning/phases/06-extension-contracts-performance/06-01-PLAN.md
@projects/cli/cli/config.py
@projects/cli/cli/main.py
@projects/cli/README.md
@projects/cli/settings_mythic.yaml
@projects/cli/settings_outflank.yaml
@projects/cli/settings_cobaltstrike.yaml
@projects/cli/tests/test_config.py
@projects/cli/tests/test_sync.py
@docs/performance.md
@projects/file_enrichment/tests/benchmarks/README.md
@projects/file_enrichment/tests/benchmarks/bench_basic_analysis.py
</context>

<tasks>

## Task 1
<task type="auto">
  <name>Task 1: Add connector config preflight validation contract and schema-key alignment</name>
  <files>projects/cli/cli/config.py, projects/cli/cli/main.py, projects/cli/settings_mythic.yaml, projects/cli/settings_outflank.yaml, projects/cli/settings_cobaltstrike.yaml</files>
  <action>Introduce/strengthen explicit connector preflight validation behavior (including clear failure output) and align settings examples to schema field names and required connector metadata so miswiring is caught before runtime sync loops.</action>
  <verify>cd projects/cli && uv run pytest tests/test_config.py tests/test_sync.py -q</verify>
  <done>Connector onboarding has a deterministic, command-backed validation contract with schema-aligned templates.</done>
</task>

## Task 2
<task type="auto">
  <name>Task 2: Publish connector onboarding validation runbook in CLI docs</name>
  <files>projects/cli/README.md</files>
  <action>Add a dedicated onboarding section that defines validation-first flow (config edit, preflight validate, connector start), common misconfiguration signatures, and remediation guidance mapped to connector type.</action>
  <verify>rg -n "preflight|validate config|connect-outflank|connect-mythic|connect-cobaltstrike|downloads_dir_path" projects/cli/README.md projects/cli/settings_outflank.yaml</verify>
  <done>Connector docs prevent runtime miswiring by requiring preflight validation and schema-aligned examples.</done>
</task>

## Task 3
<task type="auto">
  <name>Task 3: Add workflow-throughput baseline and tuning guardrail guidance</name>
  <files>docs/performance.md, projects/file_enrichment/tests/benchmarks/README.md, projects/file_enrichment/tests/benchmarks/bench_basic_analysis.py</files>
  <action>Document a measurable throughput baseline workflow (benchmark-only run, baseline save, compare) and tie tuning recommendations to specific guardrail metrics while clearly distinguishing micro-benchmark results from end-to-end workflow claims.</action>
  <verify>cd projects/file_enrichment && uv run pytest tests/benchmarks/bench_basic_analysis.py --benchmark-only</verify>
  <done>Throughput tuning is grounded in repeatable benchmark commands and explicit guardrail interpretation rules.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd projects/cli && uv run pytest tests/test_config.py tests/test_sync.py -q` passes
- [ ] `cd projects/file_enrichment && uv run pytest tests/benchmarks/bench_basic_analysis.py --benchmark-only` passes
- [ ] `rg -n "preflight|validate config|downloads_dir_path|connect-outflank|connect-mythic|connect-cobaltstrike" projects/cli/README.md projects/cli/settings_outflank.yaml projects/cli/cli/config.py` confirms connector onboarding contract alignment
- [ ] `rg -n "benchmark-save|benchmark-compare|throughput|ENRICHMENT_MAX_PARALLEL_WORKFLOWS|DOCUMENTCONVERSION_MAX_PARALLEL_WORKFLOWS" docs/performance.md projects/file_enrichment/tests/benchmarks/README.md` confirms measurable baseline/tuning guardrails
- [ ] EXT-02 evidence recorded: connector onboarding schema/config validation guidance prevents runtime miswiring
</verification>

<success_criteria>

- EXT-02 is satisfied by connector onboarding validation guidance and schema-aligned examples
- Workflow-throughput baseline/tuning outcomes are measurable via documented benchmark commands
- Performance guidance remains compatible with existing architecture and avoids unsupported claims

</success_criteria>

<output>
After completion, create `.planning/phases/06-extension-contracts-performance/06-02-SUMMARY.md`
</output>
